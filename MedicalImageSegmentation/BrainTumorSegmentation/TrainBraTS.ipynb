{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GAN BraTS 2022\n* filtre = 24\n* n_patch = 1\n* lr = dynamic\n* alpha = 5","metadata":{}},{"cell_type":"code","source":"! pip install elasticdeform\n! pip install tensorflow_addons\n! pip install gdown","metadata":{"id":"4jwTkCT08Q13","outputId":"6f9ea1cd-70bb-4f2d-ee6e-3909d23e2aec","execution":{"iopub.status.busy":"2022-05-01T04:58:47.957721Z","iopub.execute_input":"2022-05-01T04:58:47.958073Z","iopub.status.idle":"2022-05-01T04:59:33.520327Z","shell.execute_reply.started":"2022-05-01T04:58:47.95798Z","shell.execute_reply":"2022-05-01T04:59:33.519373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#IMPORTS\nimport gdown\nimport tensorflow as tf\nimport numpy as np\nimport tarfile\nimport nibabel as nib\nimport glob\nimport time\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom sys import stdout\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpim\nfrom scipy.ndimage.interpolation import affine_transform\nfrom sklearn.model_selection import train_test_split\nimport json\nimport numpy as np\nfrom scipy.ndimage.interpolation import affine_transform\nimport elasticdeform\nimport multiprocessing as mp\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport nibabel as nib\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Flatten, Conv3D, Conv3DTranspose, Dropout, ReLU, LeakyReLU, Concatenate, ZeroPadding3D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MeanSquaredError\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.layers import InstanceNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nimport numpy as np\nfrom scipy import ndimage\nfrom sklearn.metrics import precision_score\nfrom tqdm import tqdm  \nfrom time import sleep \nimport math\nfrom keras import backend as K","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#IMPORT INPUT FOLDER FROM DRIVE\ninputs_file = 'https://drive.google.com/file/d/1eSUN5_gtCkpPn277GpLe-Pr_suiYBtWU/view?usp=sharing'\ninputs_url = 'https://drive.google.com/uc?id=1eSUN5_gtCkpPn277GpLe-Pr_suiYBtWU'\n\ninputs_output = 'inputs.zip'\ngdown.download(inputs_url, inputs_output, quiet=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T04:59:33.523638Z","iopub.execute_input":"2022-05-01T04:59:33.523944Z","iopub.status.idle":"2022-05-01T04:59:34.632995Z","shell.execute_reply.started":"2022-05-01T04:59:33.523902Z","shell.execute_reply":"2022-05-01T04:59:34.632339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! unzip ./inputs.zip -d ./INPUTS\n! mv -v ./INPUTS/content/drive/MyDrive/GANs/BrainTumorDetection/INPUTS_2021/* ./INPUTS\n! rm -rf ./INPUTS/content\n! rm ./inputs.zip","metadata":{"execution":{"iopub.status.busy":"2022-05-01T04:59:34.635176Z","iopub.execute_input":"2022-05-01T04:59:34.635607Z","iopub.status.idle":"2022-05-01T04:59:37.408518Z","shell.execute_reply.started":"2022-05-01T04:59:34.635566Z","shell.execute_reply":"2022-05-01T04:59:37.407478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Eeach execution has 5 epochs \n#kaggle can run 12 hours at a time\nEXEC = 59\nVERSION = 'brats_22'\nINPUT_PATH = './INPUTS'\nPATH = './RESULTS' \nDATA_PATH = './data'","metadata":{"id":"pkjo9C7Ah9bZ","execution":{"iopub.status.busy":"2022-05-01T04:59:37.414226Z","iopub.execute_input":"2022-05-01T04:59:37.416169Z","iopub.status.idle":"2022-05-01T04:59:37.422808Z","shell.execute_reply.started":"2022-05-01T04:59:37.416128Z","shell.execute_reply":"2022-05-01T04:59:37.42182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import result folder  from previous epoch to continue the training\nif EXEC!=1:\n    results_file = 'https://drive.google.com/file/d/15mb9qMku7cvycVYrY02Ifvh_LkA_zEqG/view?usp=sharing'\n    results_url = 'https://drive.google.com/uc?id=15mb9qMku7cvycVYrY02Ifvh_LkA_zEqG'\n\n    results_output = 'results.zip'\n\n    gdown.download(results_url, results_output, quiet=False)\n    \nelse:\n    ! mkdir ./RESULTS","metadata":{"execution":{"iopub.status.busy":"2022-05-01T04:59:37.427158Z","iopub.execute_input":"2022-05-01T04:59:37.428289Z","iopub.status.idle":"2022-05-01T04:59:38.178132Z","shell.execute_reply.started":"2022-05-01T04:59:37.428247Z","shell.execute_reply":"2022-05-01T04:59:38.177084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if EXEC!=1:\n    ! unzip ./results.zip -d ./RESULTS\n    ! mv -v ./RESULTS/content/drive/MyDrive/GANs/BrainTumorDetection/RESULTS_KAGGLE/VERSION_$VERSION/RESULTS_{EXEC-1}/* ./RESULTS\n    ! rm -rf ./RESULTS/content\n    ! rm ./results.zip  ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T04:59:38.179715Z","iopub.execute_input":"2022-05-01T04:59:38.179982Z","iopub.status.idle":"2022-05-01T04:59:38.19032Z","shell.execute_reply.started":"2022-05-01T04:59:38.179943Z","shell.execute_reply":"2022-05-01T04:59:38.189257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PATCH EXTRACTION AND AUGMENTATION \n\ndef patch_extraction(Xb, yb, sizePatches=128, Npatches=1, max_tries = 50, background_threshold=0.95, num_classes=4, apply_ratio=False):\n    \"\"\"\n    3D patch extraction\n    \"\"\"\n    \n    batch_size, rows, columns, slices, channels = Xb.shape\n    X_patches = np.empty((batch_size*Npatches, sizePatches, sizePatches, sizePatches, channels))\n    y_patches = np.empty((batch_size*Npatches, sizePatches, sizePatches, sizePatches))\n    i = 0\n    list_id = []\n    \n    if apply_ratio:\n      #print(f'Application of ratio(bg_ratio < {background_threshold*100}%): {apply_ratio}')\n      for b in range(batch_size):\n          for p in range(Npatches):\n              \n              temp_X = None\n              temp_y = None\n\n              tries = 0\n              out = False    \n              while (tries < max_tries) and (not out):\n                #choose a random position from which the patch starts\n                x = np.random.randint(rows-sizePatches+1) \n                y = np.random.randint(columns-sizePatches+1)\n                z = np.random.randint(slices-sizePatches+1) \n\n                # extract the brain patch\n \n                temp_X = Xb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches, :]\n\n                # extract GT\n                temp_y = yb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches]\n\n                # One-hot Encoding\n                temp_y_cat = keras.utils.to_categorical(temp_y, num_classes=num_classes)\n                \n               \n                bgrd_ratio = np.sum(temp_y_cat[:, :, :, 0])/(sizePatches * sizePatches * sizePatches)\n\n                tries += 1\n\n                if bgrd_ratio < background_threshold:\n                  out = True\n                  temp_X = Xb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches, :]\n\n              #print(f'\\nBG_ratio : {bgrd_ratio*100} %')\n              #print(f\"\\nTried {tries} times to find a sub-volume. Giving up...\")\n              X_patches[i] = temp_X\n              y_patches[i] = temp_y\n              i += 1\n\n    else:\n      for b in range(batch_size):\n        for p in range(Npatches):\n            x = np.random.randint(rows-sizePatches+1) \n            y = np.random.randint(columns-sizePatches+1)\n            z = np.random.randint(slices-sizePatches+1) \n\n            X_patches[i] = Xb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches, :]\n            y_patches[i] = yb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches]\n            i += 1\n                              \n    return X_patches, y_patches\n\ndef flip3D(X, y):\n    \"\"\"\n    Flip the 3D image respect one of the 3 axis chosen randomly\n    \"\"\"\n    choice = np.random.randint(3)\n    if choice == 0: # flip on x\n        X_flip, y_flip = X[::-1, :, :, :], y[::-1, :, :]\n    if choice == 1: # flip on y\n        X_flip, y_flip = X[:, ::-1, :, :], y[:, ::-1, :]\n    if choice == 2: # flip on z\n        X_flip, y_flip = X[:, :, ::-1, :], y[:, :, ::-1]\n        \n    return X_flip, y_flip\n\n\ndef rotation3D(X, y):\n    \"\"\"\n    Rotate a 3D image with alfa, beta and gamma degree respect the axis x, y and z respectively.\n    The three angles are chosen randomly between 0-30 degrees\n    \"\"\"\n    alpha, beta, gamma = np.pi*np.random.random_sample(3,)/6\n    Rx = np.array([[1, 0, 0],\n                   [0, np.cos(alpha), -np.sin(alpha)],\n                   [0, np.sin(alpha), np.cos(alpha)]])\n    \n    Ry = np.array([[np.cos(beta), 0, np.sin(beta)],\n                   [0, 1, 0],\n                   [-np.sin(beta), 0, np.cos(beta)]])\n    \n    Rz = np.array([[np.cos(gamma), -np.sin(gamma), 0],\n                   [np.sin(gamma), np.cos(gamma), 0],\n                   [0, 0, 1]])\n    \n    R = np.dot(np.dot(Rx, Ry), Rz)\n    \n    X_rot = np.empty_like(X)\n    for channel in range(X.shape[-1]):\n        X_rot[:,:,:,channel] = affine_transform(X[:,:,:,channel], R, offset=0, order=3, mode='constant')\n    y_rot = affine_transform(y, R, offset=0, order=0, mode='constant')\n    \n    del(Rx, Ry, Rz, R)\n    return X_rot, y_rot\n\ndef brightness(X, y):\n    \"\"\"\n    Changing the brighness of a image using power-law gamma transformation.\n    Gain and gamma are chosen randomly for each image channel.\n    \n    Gain chosen between [0.9 - 1.1]\n    Gamma chosen between [0.9 - 1.1]\n    \n    new_im = gain * im^gamma\n    \"\"\"\n    \n    X_new = np.zeros(X.shape)\n    for c in range(X.shape[-1]):\n        im = X[:,:,:,c]        \n        gain, gamma = (1.2 - 0.8) * np.random.random_sample(2,) + 0.8\n        im_new = np.sign(im)*gain*(np.abs(im)**gamma)\n        X_new[:,:,:,c] = im_new \n        \n    del(im, gain, gamma, im_new)\n    return X_new, y\n\ndef elastic(X, y):\n    \"\"\"\n    Elastic deformation on a image and its target\n    \"\"\"  \n    [Xel, yel] = elasticdeform.deform_random_grid([X, y], sigma=2, axis=[(0, 1, 2), (0, 1, 2)], order=[1, 0], mode='constant')\n    \n    return Xel, yel\n\ndef random_decisions(N):\n    \"\"\"\n    Generate N random decisions for augmentation\n    N should be equal to the batch size\n    \"\"\"\n    \n    decisions = np.zeros((N, 4)) # 4 is number of aug techniques to combine (patch extraction excluded)\n    for n in range(N):\n        decisions[n] = np.random.randint(2, size=4)\n        \n    return decisions\n\ndef combine_aug(X, y, do):\n    \"\"\"\n    Combine randomly the different augmentation techniques written above\n    \"\"\"\n    Xnew, ynew = X, y\n    \n    # make sure to use at least the 25% of original images\n    if np.random.random_sample()>0.75:\n        return Xnew, ynew\n    \n    else:   \n        if do[0] == 1:\n            Xnew, ynew = flip3D(Xnew, ynew)\n\n        if do[1] == 1:\n            Xnew, ynew = brightness(Xnew, ynew)   \n\n        if do[2] == 1:\n            Xnew, ynew = rotation3D(Xnew, ynew)\n\n        if do[3] == 1:\n            Xnew, ynew = elastic(Xnew, ynew)\n        \n        return Xnew, ynew\n\ndef aug_batch(Xb, Yb):\n    \"\"\"\n    Generate a augmented image batch \n    \"\"\"\n    batch_size = len(Xb)\n    newXb, newYb = np.empty_like(Xb), np.empty_like(Yb)\n    \n    decisions = random_decisions(batch_size)            \n    inputs = [(X, y, do) for X, y, do in zip(Xb, Yb, decisions)]\n    pool = mp.Pool(processes=8)\n    multi_result = pool.starmap(combine_aug, inputs)\n    pool.close()\n    \n    for i in range(len(Xb)):\n        newXb[i], newYb[i] = multi_result[i][0], multi_result[i][1]\n    \n    del(decisions, inputs, multi_result, pool)\n    return newXb, newYb ","metadata":{"id":"25O0ebVE3MLK","execution":{"iopub.status.busy":"2022-05-01T04:59:44.057734Z","iopub.execute_input":"2022-05-01T04:59:44.05813Z","iopub.status.idle":"2022-05-01T04:59:44.103379Z","shell.execute_reply.started":"2022-05-01T04:59:44.058047Z","shell.execute_reply":"2022-05-01T04:59:44.102162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load image and data generator \n#generate train set and valid set\n\ndef load_img(img_files):\n    ''' Load one image and its target form file\n    '''\n    N = len(img_files)\n    # target\n    y = nib.load(img_files[N-1]).get_fdata(dtype='float32', caching='unchanged')\n    y = y[40:200,34:226,8:136]\n    y[y==4]=3\n      \n    X_norm = np.empty((240, 240, 155, 4))\n    for channel in range(N-1):\n        X = nib.load(img_files[channel]).get_fdata(dtype='float32', caching='unchanged')\n        brain = X[X!=0] \n        brain_norm = np.zeros_like(X) \n        # background at -100\n        norm = (brain - np.mean(brain))/np.std(brain)\n        brain_norm[X!=0] = norm\n        X_norm[:,:,:,channel] = brain_norm        \n        \n    X_norm = X_norm[40:200,34:226,8:136,:]    \n    del(X, brain, brain_norm)\n    \n    return X_norm, y\n    \n    \nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, batch_size=4, dim=(160,192,128), n_channels=4, n_classes=4, shuffle=True, augmentation=False, patch_size=128, n_patches=1):\n        'Initialization'\n        self.list_IDs = list_IDs\n        self.batch_size = batch_size\n        self.dim = dim\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.augmentation = augmentation\n        self.patch_size = patch_size\n        self.n_patches = n_patches\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return len(self.list_IDs) // self.batch_size\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data     \n        X, y = self.__data_generation(list_IDs_temp)\n        if self.augmentation == True:\n            X, y = self.__data_augmentation(X, y)\n        \n        if index == self.__len__()-1:\n            self.on_epoch_end()\n        \n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n  \n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size, *self.dim))\n\n        # Generate data\n        for i, IDs in enumerate(list_IDs_temp):\n            # Store sample\n            X[i], y[i] = load_img(IDs)\n            \n        if self.augmentation == True:\n            return X.astype('float32'), y\n        else:\n            return X.astype('float32'), to_categorical(y, self.n_classes)\n\n    def __data_augmentation(self, X, y):\n        'Apply augmentation'\n        X_aug, y_aug = patch_extraction(X, y, sizePatches=self.patch_size, Npatches=self.n_patches, apply_ratio=False)\n        X_aug, y_aug = aug_batch(X_aug, y_aug)\n                \n        return X_aug, to_categorical(y_aug, self.n_classes)","metadata":{"id":"4tjALFSnzl4S","execution":{"iopub.status.busy":"2022-05-01T04:59:44.104735Z","iopub.execute_input":"2022-05-01T04:59:44.104988Z","iopub.status.idle":"2022-05-01T04:59:44.128267Z","shell.execute_reply.started":"2022-05-01T04:59:44.104953Z","shell.execute_reply":"2022-05-01T04:59:44.127552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MODELS Unet3DGan\n\ndef double_conv(x, Nf, ks, norm=True):\n      for ss in range(2):\n          x = Conv3D(Nf, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n          \n          if (norm):\n              x = BatchNormalization()(x) \n          x = ReLU()(x) \n      return x   \n\ndef Generator():\n    '''\n    Generator model\n    '''\n    \n    def encoder_step(layer, Nf, ks, norm=True):\n        x = double_conv(layer, Nf/2, ks, norm)\n        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(x)  \n        x = Dropout(0.1)(x) \n\n        return x\n\n    def bottlenek(layer, Nf, ks):\n        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n        x = BatchNormalization()(x)\n        x = ReLU()(x)\n        \n        for i in range(4):\n            y = Conv3D(Nf, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n            x = BatchNormalization()(y)\n            x = ReLU()(x)\n            x = Concatenate()([x, y])\n#             print(x.shape)\n        return x\n\n    def decoder_step(layer, layer_to_concatenate, Nf, ks):\n        \n        x = Dropout(0.1)(layer)\n        x = Conv3DTranspose(Nf, kernel_size=ks, strides=2, padding='same', kernel_initializer='he_normal')(x)\n        x = Concatenate()([x, layer_to_concatenate])\n        x = double_conv(x, Nf, ks)\n        return x\n\n    layers_to_concatenate = []\n    inputs = Input((128,128,128,4), name='input_image')\n    Nfilter_start = 24\n    depth = 5\n    ks = 4\n    x = inputs\n\n    # encoder\n    for d in range(depth):\n        if d==0: \n          x = Conv3D(Nfilter_start, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n          x = Conv3D(Nfilter_start, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n        else:\n          x = encoder_step(x, Nfilter_start*np.power(2,d), ks)\n        \n        layers_to_concatenate.append(x)\n\n    # bottlenek\n    x = bottlenek(x, Nfilter_start*np.power(2,depth-1), ks)\n    \n    # decoder\n    for d in range(depth-2, -1, -1): \n        x = decoder_step(x, layers_to_concatenate.pop(), Nfilter_start*np.power(2,d), ks)\n    \n    # classifier\n    last = Conv3DTranspose(4, kernel_size=ks, strides=2, padding='same', kernel_initializer='he_normal', activation='softmax', name='output_generator')(x)\n    \n    del(x,layers_to_concatenate)\n    return Model(inputs=inputs, outputs=last, name='Generator')\n\ndef Discriminator():\n    '''\n    Discriminator model\n    '''\n\n    inputs = Input((128,128,128,4), name='input_image')\n    targets = Input((128,128,128,4), name='target_image')\n    Nfilter_start = 24\n    depth = 3\n    ks = 4\n\n    def encoder_step(layer, Nf, norm=True):\n        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n        if norm:\n            x = BatchNormalization()(x)\n        x = LeakyReLU()(x)\n        x = Dropout(0.1)(x)\n        \n        return x\n\n    x = Concatenate()([inputs, targets])\n\n    for d in range(depth+1):\n        if d==0:\n            x = encoder_step(x, Nfilter_start*np.power(2,d), False)\n        else:\n            x = encoder_step(x, Nfilter_start*np.power(2,d))\n            \n\n    last = Conv3D(1, ks, strides=1, padding='valid', kernel_initializer='he_normal', name='output_discriminator')(x) \n    \n    del(x)\n    return Model(inputs=[targets, inputs], outputs=last, name='Discriminator')\n\ndef ensembler():\n\n    start = Input((128,128,128,40))\n    fin = Conv3D(4, kernel_size=3, kernel_initializer='he_normal', padding='same', activation='softmax')(start)\n\n    return Model(inputs=start, outputs=fin, name='Ensembler')\n    \n","metadata":{"id":"4lDxXmP44Dfq","execution":{"iopub.status.busy":"2022-05-01T04:59:44.259878Z","iopub.execute_input":"2022-05-01T04:59:44.260606Z","iopub.status.idle":"2022-05-01T04:59:44.28401Z","shell.execute_reply.started":"2022-05-01T04:59:44.260569Z","shell.execute_reply":"2022-05-01T04:59:44.283367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calcul losses\n\n\ndef diceLoss(y_true, y_pred, class_weights):\n    y_true = tf.convert_to_tensor(y_true, 'float32')\n    y_pred = tf.convert_to_tensor(y_pred, y_true.dtype)\n\n    num = tf.math.reduce_sum(tf.math.multiply(class_weights, tf.math.reduce_sum(tf.math.multiply(y_true, y_pred), axis=[0,1,2,3])))\n    den = tf.math.reduce_sum(tf.math.multiply(class_weights, tf.math.reduce_sum(tf.math.add(y_true, y_pred), axis=[0,1,2,3])))+1e-5\n    return 1-2*num/den\n\ndef discriminator_loss(disc_real_output, disc_fake_output):\n    real_loss = tf.math.reduce_mean(tf.math.pow(tf.ones_like(disc_real_output) - disc_real_output, 2))\n    fake_loss = tf.math.reduce_mean(tf.math.pow(tf.zeros_like(disc_fake_output) - disc_fake_output, 2))\n\n    disc_loss = 0.5*(real_loss + fake_loss)\n\n    return disc_loss\n\n\ndef generator_loss(target, gen_output, disc_fake_output, class_weights, alpha):\n    \n    # generalized dice loss\n    dice_loss = diceLoss(target, gen_output, class_weights)\n    \n    # disc loss\n    disc_loss = tf.math.reduce_mean(tf.math.pow(tf.ones_like(disc_fake_output) - disc_fake_output, 2))\n   \n    # total loss\n    gen_loss = alpha*dice_loss + disc_loss\n   \n\n    return gen_loss, dice_loss, disc_loss","metadata":{"id":"EopAuIWScX8C","execution":{"iopub.status.busy":"2022-05-01T04:59:44.285232Z","iopub.execute_input":"2022-05-01T04:59:44.28554Z","iopub.status.idle":"2022-05-01T04:59:44.297588Z","shell.execute_reply.started":"2022-05-01T04:59:44.285471Z","shell.execute_reply":"2022-05-01T04:59:44.296929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# *  **WT = ET ∪ ED ∪ NCR/NET**\n# *  **TC = ET ∪ NCR/NET**\n# *  **ET = ET**\n\n\n\n1.   ED (edema -> classe 2)\n2.   ET (enhancing_tumor -> classe 3)\n3.   NCR/NET (non_enhancing_tumor -> classe 1)\n4.   BG (background -> classe 0)\n\n\n\n\n\n","metadata":{"id":"ntkNQlCr-t_J"}},{"cell_type":"code","source":"#METRICS EVALUATING\n\n\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n    intersection = np.sum(y_true_f * y_pred_f)\n    smooth = 1e-5\n\n    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n\ndef binary_dice3d(s,g):\n    #dice score of two 3D volumes\n    smooth = 1e-5\n    num=np.sum(np.multiply(s, g))\n    denom=s.sum() + g.sum() + smooth\n    \n    return  2.0*num/denom\n\ndef sensitivity (seg,ground): \n    #computs false negative rate\n    smooth = 1e-5\n    num=np.sum(np.multiply(ground, seg))\n    denom=np.sum(ground)+smooth\n    \n    return  num/denom\n\ndef specificity (seg,ground): \n    #computes false positive rate\n    smooth = 1e-5\n    num=np.sum(np.multiply(ground==0, seg ==0))\n    denom=np.sum(ground==0)+smooth\n    \n    return  num/denom\n\ndef border_map(binary_img,neigh):\n    \"\"\"\n    Creates the border for a 3D image\n    \"\"\"\n    binary_map = np.asarray(binary_img, dtype=np.uint8)\n    neigh = neigh\n    west = ndimage.shift(binary_map, [-1, 0,0], order=0)\n    east = ndimage.shift(binary_map, [1, 0,0], order=0)\n    north = ndimage.shift(binary_map, [0, 1,0], order=0)\n    south = ndimage.shift(binary_map, [0, -1,0], order=0)\n    top = ndimage.shift(binary_map, [0, 0, 1], order=0)\n    bottom = ndimage.shift(binary_map, [0, 0, -1], order=0)\n    cumulative = west + east + north + south + top + bottom\n    border = ((cumulative < 6) * binary_map) == 1\n    return border\n\ndef border_distance(ref,seg):\n    \"\"\"\n    This functions determines the map of distance from the borders of the\n    segmentation and the reference and the border maps themselves\n    \"\"\"\n    neigh=8\n    border_ref = border_map(ref,neigh)\n    border_seg = border_map(seg,neigh)\n    oppose_ref = 1 - ref\n    oppose_seg = 1 - seg\n    # euclidean distance transform\n    distance_ref = ndimage.distance_transform_edt(oppose_ref)\n    distance_seg = ndimage.distance_transform_edt(oppose_seg)\n    distance_border_seg = border_ref * distance_seg\n    distance_border_ref = border_seg * distance_ref\n    return distance_border_ref, distance_border_seg#, border_ref, border_seg\n\ndef Hausdorff_distance(ref,seg):\n    \"\"\"\n    This functions calculates the average symmetric distance and the\n    hausdorff distance between a segmentation and a reference image\n    :return: hausdorff distance and average symmetric distance\n    \"\"\"\n    ref_border_dist, seg_border_dist = border_distance(ref,seg)\n    hausdorff_distance = np.max([np.max(ref_border_dist), np.max(seg_border_dist)])\n    return hausdorff_distance\n\ndef DSC_whole(pred, orig_label):\n    #computes dice for the whole tumor\n    return dice_coef(pred>0,orig_label>0)\n\ndef DSC_en(pred, orig_label):\n    #computes dice for enhancing region\n    return dice_coef(pred==3,orig_label==3)\n\ndef DSC_core(pred, orig_label):\n    #computes dice for core region\n    seg_=np.copy(pred)\n    ground_=np.copy(orig_label)\n    seg_[seg_==2]=0\n    ground_[ground_==2]=0\n    return dice_coef(seg_>0,ground_>0)\n\ndef sensitivity_whole (seg,ground):\n    return sensitivity(seg>0,ground>0)\n\ndef sensitivity_en (seg,ground):\n    return sensitivity(seg==3,ground==3)\n\ndef sensitivity_core (seg,ground):\n    seg_=np.copy(seg)\n    ground_=np.copy(ground)\n    seg_[seg_==2]=0\n    ground_[ground_==2]=0\n    return sensitivity(seg_>0,ground_>0)\n\ndef specificity_whole (seg,ground):\n    return specificity(seg>0,ground>0)\n\ndef specificity_en (seg,ground):\n    return specificity(seg==3,ground==3)\n\ndef specificity_core (seg,ground):\n    seg_=np.copy(seg)\n    ground_=np.copy(ground)\n    seg_[seg_==2]=0\n    ground_[ground_==2]=0\n    return specificity(seg_>0,ground_>0)\n    \ndef hausdorff_whole (seg,ground):\n    return Hausdorff_distance(seg==0,ground==0)\n\ndef hausdorff_en (seg,ground):\n    return Hausdorff_distance(seg!=3,ground!=3)\n\ndef hausdorff_core (seg,ground):\n    seg_=np.copy(seg)\n    ground_=np.copy(ground)\n    seg_[seg_==2]=0\n    ground_[ground_==2]=0\n    return Hausdorff_distance(seg_==0,ground_==0)","metadata":{"id":"UpgY_tqqVgMj","execution":{"iopub.status.busy":"2022-05-01T04:59:44.300412Z","iopub.execute_input":"2022-05-01T04:59:44.300758Z","iopub.status.idle":"2022-05-01T04:59:44.325843Z","shell.execute_reply.started":"2022-05-01T04:59:44.30072Z","shell.execute_reply":"2022-05-01T04:59:44.324998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass_weights = np.load(INPUT_PATH + '/class_weights.npy')\n\n# Models\nG = Generator()\nD = Discriminator()\n\nif os.path.exists(PATH + f'/{VERSION}_Generator.h5')==True and os.path.exists(PATH + f'/{VERSION}_Discriminator.h5')==True: \n    G.load_weights(PATH + f'/{VERSION}_Generator.h5')\n    D.load_weights(PATH + f'/{VERSION}_Discriminator.h5')\n\n# Optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)\n\n\n\n@tf.function\ndef train_step(image, target, alpha):\n  \n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n\n      gen_output = G(image, training=True)\n      disc_real_output = D([image, target], training=True)\n      disc_fake_output = D([image, gen_output], training=True)\n      disc_loss = discriminator_loss(disc_real_output, disc_fake_output)\n      \n      gen_loss, dice_loss, disc_loss_gen = generator_loss(target, gen_output, disc_fake_output, class_weights, alpha)\n      del gen_output, disc_fake_output,image\n  generator_gradients = gen_tape.gradient(gen_loss, G.trainable_variables)\n  discriminator_gradients = disc_tape.gradient(disc_loss, D.trainable_variables)\n\n  generator_optimizer.apply_gradients(zip(generator_gradients, G.trainable_variables))\n  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, D.trainable_variables))\n      \n  return gen_loss, dice_loss, disc_loss_gen\n        \n\n\ndef fit(train_gen, valid_gen, alpha, epochs, jump):\n    \n    if os.path.exists(PATH)==False:\n        os.mkdir(PATH)\n        \n    Nt = len(train_gen)\n    history = {'train': [], 'valid': []}\n    \n    stats={'dice':[],'spec':[],'sen':[],'hau95':[]}\n    prev_loss = np.inf\n    \n    epoch_ugan_loss = tf.keras.metrics.Mean()\n    epoch_dice_loss = tf.keras.metrics.Mean()\n    epoch_disc_loss = tf.keras.metrics.Mean()\n    \n    \n    ugan_min = 2\n    \n    for e in range(epochs):\n        print('Epoch {}/{}'.format(e+1,epochs))\n        b = 0\n        \n        x=(EXEC -1)*EPOCHS + e\n        lrg=(math.exp(-x))*0.001 + 0.00001\n        lrd=(math.exp(-x))*0.001 + 0.00002\n        K.set_value(generator_optimizer.learning_rate, lrg)\n        K.set_value(discriminator_optimizer.learning_rate, lrd)\n        \n        for Xb, yb in tqdm(train_gen):\n\n          b += 1 \n          losses = train_step(Xb, yb, alpha)\n          epoch_ugan_loss.update_state(losses[0])\n          epoch_dice_loss.update_state(losses[1])\n          epoch_disc_loss.update_state(losses[2])\n          \n          print('\\n Batch: {}/{} - loss: {:.4f} - dice_loss: {:.4f} - disc_loss: {:.4f}'\n                        .format(b, Nt, epoch_ugan_loss.result(), epoch_dice_loss.result(), epoch_disc_loss.result()))\n          \n#           stdout.flush()\n        del Xb,yb    \n        history['train'].append([epoch_ugan_loss.result(), epoch_dice_loss.result(), epoch_disc_loss.result()])\n        \n       \n       \n        try :\n           \n                G.save_weights(PATH + f'/{VERSION}_Generator.h5') \n                D.save_weights(PATH + f'/{VERSION}_Discriminator.h5')\n                \n        except:\n            print('Weights not saved')\n\n        \n        epoch_ugan_loss.reset_states()\n        epoch_dice_loss.reset_states()\n        epoch_disc_loss.reset_states()\n        \n        \n       \n        \n    return history","metadata":{"id":"booFxn3o7Mcg","execution":{"iopub.status.busy":"2022-05-01T04:59:44.327375Z","iopub.execute_input":"2022-05-01T04:59:44.32794Z","iopub.status.idle":"2022-05-01T04:59:47.465479Z","shell.execute_reply.started":"2022-05-01T04:59:44.327902Z","shell.execute_reply":"2022-05-01T04:59:47.464731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir data","metadata":{"execution":{"iopub.status.busy":"2022-05-01T04:59:47.466815Z","iopub.execute_input":"2022-05-01T04:59:47.467049Z","iopub.status.idle":"2022-05-01T04:59:48.14147Z","shell.execute_reply.started":"2022-05-01T04:59:47.467016Z","shell.execute_reply":"2022-05-01T04:59:48.14007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_file = tarfile.open(\"../input/brats-2021-task1/BraTS2021_Training_Data.tar\")\nzip_file.extractall(DATA_PATH)\nzip_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T04:59:48.146171Z","iopub.execute_input":"2022-05-01T04:59:48.146573Z","iopub.status.idle":"2022-05-01T05:02:07.891418Z","shell.execute_reply.started":"2022-05-01T04:59:48.14653Z","shell.execute_reply":"2022-05-01T05:02:07.890666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t1_list = sorted(glob.glob(DATA_PATH+'/*/*_t1.nii.gz'))\nt2_list = sorted(glob.glob(DATA_PATH+'/*/*_t2.nii.gz'))\nt1ce_list = sorted(glob.glob(DATA_PATH+'/*/*_t1ce.nii.gz'))\nflair_list = sorted(glob.glob(DATA_PATH+'/*/*_flair.nii.gz'))\nseg_list = sorted(glob.glob(DATA_PATH+'/*/*_seg.nii.gz'))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:04:03.840206Z","iopub.execute_input":"2022-05-01T05:04:03.840752Z","iopub.status.idle":"2022-05-01T05:04:04.01863Z","shell.execute_reply.started":"2022-05-01T05:04:03.840716Z","shell.execute_reply":"2022-05-01T05:04:04.017849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(t1_list),len(t2_list),len(t1ce_list),len(flair_list),len(seg_list))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:02:08.072811Z","iopub.execute_input":"2022-05-01T05:02:08.073056Z","iopub.status.idle":"2022-05-01T05:02:08.078898Z","shell.execute_reply.started":"2022-05-01T05:02:08.073022Z","shell.execute_reply":"2022-05-01T05:02:08.078275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#*******************MAIN**********************************************************\n#*********************************************************************************\n#EXTRACTION OF DATA , T1 T2 T1C TFLAIR GROUNDTRUE\nBATCH_SIZE = 4\nALPHA = 5\nEPOCHS = 5\nNB_CLASSES = 4\nN_PATCHES = 1\n\nJUMP = False\npred_img = (PATH + f'/{VERSION}'+'_pred@epoch_{:03d}.png').format((EXEC-1)*EPOCHS) #0,8,16\nif os.path.exists(pred_img)==True:\n    JUMP = True\n\n# create the training and validation sets or load it from inputs\n#Added here\nNim = len(t1_list)\n\nidx = np.arange(Nim)\nsets = {'train': [], 'valid': []}\n#Train in 100% of data to participate to BraTS witch provide validation data without GT\nfor i in range(len(t1_list)):\n    sets['train'].append([t1_list[i], t2_list[i], t1ce_list[i], flair_list[i], seg_list[i]])\n\n\ndel(t1_list,t2_list,t1ce_list,flair_list,seg_list,idx,Nim)\n\ntrain_gen = DataGenerator(sets['train'], batch_size=BATCH_SIZE, n_classes=NB_CLASSES, n_patches=N_PATCHES, augmentation=True)\n# valid_gen = DataGenerator(sets['valid'], batch_size=BATCH_SIZE, n_classes=NB_CLASSES, n_patches=N_PATCHES, augmentation=True)\n\n","metadata":{"id":"B2Bn5bF35LTG","execution":{"iopub.status.busy":"2022-05-01T05:03:55.301837Z","iopub.execute_input":"2022-05-01T05:03:55.302112Z","iopub.status.idle":"2022-05-01T05:03:55.327778Z","shell.execute_reply.started":"2022-05-01T05:03:55.302069Z","shell.execute_reply":"2022-05-01T05:03:55.326796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(sets['train']))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:04:39.977243Z","iopub.execute_input":"2022-05-01T05:04:39.977838Z","iopub.status.idle":"2022-05-01T05:04:39.982577Z","shell.execute_reply.started":"2022-05-01T05:04:39.9778Z","shell.execute_reply":"2022-05-01T05:04:39.981661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the uGAN model","metadata":{}},{"cell_type":"code","source":"%%capture cap --no-stderr\nh = fit(train_gen, 0, ALPHA, EPOCHS, JUMP) ","metadata":{"id":"jQhIFAKdn-xt","outputId":"0299b6fe-25b3-4e90-8644-021b3f6e123a","execution":{"iopub.status.busy":"2022-05-01T05:02:08.433721Z","iopub.status.idle":"2022-05-01T05:02:08.434617Z","shell.execute_reply.started":"2022-05-01T05:02:08.434365Z","shell.execute_reply":"2022-05-01T05:02:08.434392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(PATH + f'/{VERSION}_fit_output_{EXEC}.txt', 'w') as f:\n    f.write(cap.stdout)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:02:08.435862Z","iopub.status.idle":"2022-05-01T05:02:08.436287Z","shell.execute_reply.started":"2022-05-01T05:02:08.436043Z","shell.execute_reply":"2022-05-01T05:02:08.436066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture cap_h --no-stderr\nprint(h)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:02:08.43754Z","iopub.status.idle":"2022-05-01T05:02:08.43818Z","shell.execute_reply.started":"2022-05-01T05:02:08.437921Z","shell.execute_reply":"2022-05-01T05:02:08.437945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(PATH + f'/{VERSION}_h_{EXEC}.txt', 'w') as f:\n    f.write(cap_h.stdout)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:02:08.439672Z","iopub.status.idle":"2022-05-01T05:02:08.440072Z","shell.execute_reply.started":"2022-05-01T05:02:08.439853Z","shell.execute_reply":"2022-05-01T05:02:08.439875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! zip -r './results.zip' {PATH+'/*'}","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:02:08.447294Z","iopub.status.idle":"2022-05-01T05:02:08.447756Z","shell.execute_reply.started":"2022-05-01T05:02:08.447488Z","shell.execute_reply":"2022-05-01T05:02:08.447521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./results.zip')\n# then in colab : !wget \"https://....kaggle.net/...../results.zip\"","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:02:08.449126Z","iopub.status.idle":"2022-05-01T05:02:08.449696Z","shell.execute_reply.started":"2022-05-01T05:02:08.449456Z","shell.execute_reply":"2022-05-01T05:02:08.449479Z"},"trusted":true},"execution_count":null,"outputs":[]}]}